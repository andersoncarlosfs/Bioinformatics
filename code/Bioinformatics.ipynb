{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to describe the extract, transform, load (ETL) process for gene-specific data from NCBI and UniProt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with Python 2.7.14 or Python 3.6.4. However, some external libraries are necessary to be include in the python enviroment if not yet done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# uncompressing files\n",
    "def _uncompress(_path):\n",
    "    with gzip.open(_path, 'rb') as _input, open('./' + os.path.splitext(os.path.basename(_path))[0], 'w') as _output:\n",
    "        # moving and renaming files\n",
    "        shutil.copyfileobj(_input, _output) \n",
    "        return os.path.basename(_output.name)\n",
    "    \n",
    "# reading files    \n",
    "def _read(_path):\n",
    "    return pandas.read_csv(_path, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describe the datasets used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d0b9fc8c38e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# uncompressing files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_datasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0m_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uncompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-793909f2e8c5>\u001b[0m in \u001b[0;36m_uncompress\u001b[1;34m(_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# moving and renaming files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bralt\\appdata\\local\\programs\\python\\python35-32\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mfdst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "_datasets = '../datasets/'\n",
    "\n",
    "_datasets = {\n",
    "    'Homo_sapiens.gene_info': _datasets + 'NCBI/Homo_sapiens.gene_info.gz',\n",
    "    'gene2go': _datasets + 'NCBI/gene2go.gz',\n",
    "    'UniProtKB': _datasets + 'UniProt/uniprot-cancer+AND+reviewed%3Ayes+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B--.txt.gz'\n",
    "}\n",
    "\n",
    "# uncompressing files\n",
    "for _index in _datasets:\n",
    "    _datasets[_index] = _uncompress(_datasets[_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homo_sapiens.gene_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['Homo_sapiens.gene_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gene2go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['gene2go'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target taxonomy of this project is the Homo sapiens (Human) that holds the taxon indentifier 9606."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon = 9606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniProt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UniProtKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['UniProtKB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below describe the schema of the target database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Database schema](../database/database.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration file\n",
    "_configuration = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'bioinformatics',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "with open('database.ini', 'w') as _output:\n",
    "    _output.write('[postgresql]' + '\\n')\n",
    "    # writing configuration file\n",
    "    for _parameter in _configuration:\n",
    "        _output.write(_parameter + '=' + _configuration[_parameter] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# converting any value to a string insertable into PostgreSQL\n",
    "def _convert(_data):\n",
    "    if isinstance(_data, int):\n",
    "        return str(_data)\n",
    "    if isinstance(_data, datetime):\n",
    "        # converting a date value to a string insertable into PostgreSQL\n",
    "        return '\\'' + datetime.strftime(_data, \"%Y-%m-%d\") + '\\''\n",
    "    return '\\'' + _data.strip().replace('\\'', '\\'\\'') + '\\''\n",
    "\n",
    "# skipping useless lines\n",
    "def _skip(_line, _fields):\n",
    "    _line = _line.strip()\n",
    "    # skipping comments\n",
    "    if _line.startswith('#'):\n",
    "        return None\n",
    "    # skipping comments  \n",
    "    _parts = _line.split('\\t')\n",
    "    # skipping lines with bad formatation\n",
    "    if len(_parts) != len(_fields):\n",
    "        return None\n",
    "    # skipping lines does not satisfy conditions\n",
    "    if not _parts[0].startswith(str(taxon)): \n",
    "        return None\n",
    "    return _parts\n",
    "\n",
    "# replacing values under specific conditions\n",
    "def _replace(_text, _data = None):\n",
    "    _text.strip()\n",
    "    # applying user-provided function to values\n",
    "    if isinstance(_data, tuple):\n",
    "        _text = _data[0](_text, _data[1])\n",
    "    # expading value to a list of values\n",
    "    if isinstance(_data, list):\n",
    "        for _part in _text.split('|'):\n",
    "            _data.append(_replace(_part))\n",
    "        return _data\n",
    "    # providing meaning for null values\n",
    "    if _text == '-':\n",
    "        _text = 'NOT AVAILABLE'\n",
    "    # converting any value to a string insertable into PostgreSQL\n",
    "    return _convert(_text)\n",
    "\n",
    "# applying functions to lines\n",
    "def _process(_line, _data, _fields):\n",
    "    _parts = _skip(_line, _fields)\n",
    "    # skipping useless lines\n",
    "    if _parts is None:\n",
    "        return None\n",
    "    # replacing values under specific conditions\n",
    "    for _index, _part in enumerate(_parts):\n",
    "        _data[_index] = _replace(_part, _data[_index])\n",
    "    return _data\n",
    "\n",
    "# creating SQL INSERT INTO statements\n",
    "def _strings(_data, _fields, _relation, _prefix):\n",
    "    _lines = []\n",
    "    _line = ''\n",
    "    _arrays = []\n",
    "    for _index, _value in enumerate(_data):\n",
    "        # adding values to new SQL INSERT INTO statements\n",
    "        if isinstance(_value, list):\n",
    "            _arrays.append(_index)\n",
    "        # appending values to SQL INSERT INTO statement\n",
    "        elif isinstance(_value, str):\n",
    "            _line = _line + ', ' + _value\n",
    "        else :\n",
    "            # warning the user about errors in the SQL INSERT INTO statement\n",
    "            _line = '=>' + _fields[_index] + '\\t ERROR (' + type(_value)\n",
    "            _arrays = []\n",
    "            break\n",
    "    _lines.append('INSERT INTO ' + _relation + ' VALUES (' + _line[2:] + ');\\n') \n",
    "    # adding new SQL INSERT INTO statements\n",
    "    for _index in _arrays:\n",
    "        # adding new SQL INSERT INTO statement\n",
    "        for _value in _data[_index]:\n",
    "            _lines.append('INSERT INTO ' + _fields[_index] + ' VALUES (' + _prefix + ', ' + _value + ');\\n') \n",
    "    return _lines\n",
    "\n",
    "# providing consistency to SQL INSERT INTO statements\n",
    "def _consistency(_data, _fields, _relation):\n",
    "    _line = ''\n",
    "    # appending values to SQL INSERT INTO statement\n",
    "    for _value in _data:\n",
    "        _line = _line + ', ' + _value\n",
    "    _line = 'INSERT INTO ' + _relation + ' ' + _line[2:] + ' WHERE NOT (SELECT TRUE FROM ' + _relation + ' WHERE '\n",
    "    # appending clausules to SQL INSERT INTO statement\n",
    "    for _field, _value in zip(_fields, _data):\n",
    "        _line = _line + _field + ' = ' + _value + ' AND '\n",
    "    return _line[:-5] + ');\\n'\n",
    "\n",
    "# removing duplicate lines\n",
    "def _deduplicate(_dataset, *_files):\n",
    "    _output, _path = tempfile.mkstemp()\n",
    "    for _file in _files:\n",
    "        with open(_file, 'r') as _input:\n",
    "            for _line, _group in itertools.groupby(sorted(_input)):\n",
    "                os.write(_output, _line)\n",
    "    os.close(_output)\n",
    "    shutil.move(_path, _dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_datasets['Homo_sapiens.gene_info'], 'r') as _input:\n",
    "    _fact_file, _fact_path = tempfile.mkstemp()\n",
    "    _dimension_file, _dimension_path = tempfile.mkstemp()\n",
    "    _fields = [\n",
    "        'taxonomy_id',\n",
    "        'gene_id',\n",
    "        'symbol',\n",
    "        'locus_tag',\n",
    "        'synonym',\n",
    "        'db_xref',\n",
    "        'chromosome',\n",
    "        'map_location',\n",
    "        'description',\n",
    "        'type_of_gene',\n",
    "        'symbol_from_nomenclature_authority',\n",
    "        'full_name_from_nomenclature_authority',\n",
    "        'nomenclature_status',\n",
    "        'other_designation',\n",
    "        'modification_date',\n",
    "        'feature_type'\n",
    "    ]\n",
    "    for _line in _input:\n",
    "        _data = [\n",
    "            (int, 10),\n",
    "            (int, 10),\n",
    "            None,\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [], \n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            [],\n",
    "            (datetime.strptime, '%Y%m%d'), \n",
    "            []\n",
    "        ]\n",
    "        # applying functions to lines\n",
    "        _data = _process(_line, _data, _fields)\n",
    "        if _data is None:\n",
    "            continue\n",
    "        _data.insert(0, _data.pop(1)) # gene_id\n",
    "        # creating SQL INSERT INTO statements\n",
    "        _lines = _strings(_data, _fields, 'ncbi', _data[0])\n",
    "        os.write(_fact_file, _lines.pop(0))\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_dimension_file, _line)\n",
    "    os.close(_fact_file)\n",
    "    os.close(_dimension_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['Homo_sapiens.gene_info'], _fact_path, _dimension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_datasets['gene2go'], 'r') as _input:\n",
    "    _fact_file, _fact_path = tempfile.mkstemp()\n",
    "    _dimension_file, _dimension_path = tempfile.mkstemp()\n",
    "    _fields = [\n",
    "        'taxonomy_id',\n",
    "        'gene_id',\n",
    "        'go_id',\n",
    "        'evidence',\n",
    "        'qualifier',\n",
    "        'go_term',\n",
    "        'pubmed',\n",
    "        'category'\n",
    "    ]\n",
    "    _labels = [\n",
    "        'go_id',\n",
    "        'evidence',\n",
    "        'go_term',\n",
    "        'category'\n",
    "    ]\n",
    "    for _line in _input:\n",
    "        _data = [\n",
    "            None,\n",
    "            (int, 10),\n",
    "            None,\n",
    "            [],\n",
    "            None,\n",
    "            [],\n",
    "            [],\n",
    "            None\n",
    "        ]\n",
    "        # applying functions to lines\n",
    "        _data = _process(_line, _data, _fields)\n",
    "        if _data is None:\n",
    "            continue\n",
    "        _data.pop(0) # 'taxonomy_id'\n",
    "        for _index, _value in enumerate(_data[2]): # 'evidence'\n",
    "            _data[2][_index] = _value + ', ' + _data[3] + ', ' + _replace('-') \n",
    "        _data.pop(3) # 'qualifier'\n",
    "        _data.pop(4) # 'pubmed'\n",
    "        gene_id = _data.pop(0) # 'gene_id'\n",
    "        # creating SQL INSERT INTO statements\n",
    "        _lines = _strings(_data, _labels, 'go', _data[0])\n",
    "        os.write(_fact_file, _lines.pop(0))\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_dimension_file, _line)\n",
    "        # writing SQL statements\n",
    "        for _line in _strings([gene_id, _data[0]], [_fields[1], _labels[0]], 'ncbi_go', None):\n",
    "            os.write(_dimension_file, _line)\n",
    "    os.close(_fact_file)\n",
    "    os.close(_dimension_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['gene2go'], _fact_path, _dimension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(_datasets['UniProtKB'], 'r') as _input:\n",
    "    _fact_file, _fact_path = tempfile.mkstemp()\n",
    "    _dimension_file, _dimension_path = tempfile.mkstemp()\n",
    "    _fields = [\n",
    "        'ID', \n",
    "        'AC', \n",
    "        'DE', \n",
    "        'GN', \n",
    "        'KW', \n",
    "        'DR',\n",
    "    ]\n",
    "    _labels = [\n",
    "        'protein_id',\n",
    "        'accession_number',\n",
    "        None,\n",
    "        None,\n",
    "        'keyword'\n",
    "    ]\n",
    "    _id = None\n",
    "    for _line in _input:        \n",
    "        _line = _line.strip()\n",
    "        if _line.startswith('#'):\n",
    "            continue\n",
    "        _parts = _line.split()\n",
    "        if len(_parts) < 2:\n",
    "            continue\n",
    "        _field = _parts.pop(0)\n",
    "        if _field not in _fields :\n",
    "            continue\n",
    "        _data = [_id]\n",
    "        _lines = []\n",
    "        _file = None\n",
    "        if _line[-1] == '.':\n",
    "            _line = _line[:-1]\n",
    "        if _id == None or _field == _fields[0] and _convert(_parts[0]) != _id:\n",
    "            _id = _convert(_parts[0])       \n",
    "            # creating SQL INSERT INTO statements\n",
    "            _lines = _strings([_id], [_labels[0]], 'uniprot', None)\n",
    "            _file = _fact_file\n",
    "        elif _field == _fields[1] or _field == _fields[4]:\n",
    "            _parts = _line.split(';')\n",
    "            _parts[0] = _parts[0][2:]\n",
    "            _parts[0] = _parts[0].strip()\n",
    "            _index = _fields.index(_field)\n",
    "            _label = _labels[_index]\n",
    "            for _part in _parts:\n",
    "                _part = _convert(_part)\n",
    "                if _part != '':\n",
    "                    # creating SQL INSERT INTO statements\n",
    "                    _lines = _strings([_id, _part], [_labels[0], _label], _label, None)\n",
    "        elif _field == _fields[5] and _parts.pop(0) == 'GO;':\n",
    "            _parts = _line.split(';')\n",
    "            _parts.pop(0)\n",
    "            _data = [_convert(_parts.pop(0))]\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split(':')\n",
    "            _category = _values[0].strip()\n",
    "            if _category == 'P':\n",
    "                _data.append(_convert('Process'))\n",
    "            elif _category == 'F':\n",
    "                _data.append(_convert('Function'))\n",
    "            elif _category == 'C':\n",
    "                _data.append(_convert('Component'))\n",
    "            else:\n",
    "                assert False\n",
    "            # providing consistency to SQL INSERT INTO statements\n",
    "            _lines.append(_consistency(_data, ['go_id', 'category'], 'go'))\n",
    "            _data.pop()\n",
    "            _data.append(_convert(_values[1]))\n",
    "            # providing consistency to SQL INSERT INTO statements\n",
    "            _lines.append(_consistency(_data, ['go_id', 'go_term'], 'go_term'))        \n",
    "            _data.pop()\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split(':')\n",
    "            _data.append(_convert(_values[0]))\n",
    "            _data.append(_replace('-'))\n",
    "            _data.append(_convert(_values[1]))\n",
    "            # providing consistency to SQL INSERT INTO statements\n",
    "            _lines.append(_consistency(_data, ['go_id', 'evidence', 'qualifier', 'source'], 'evidence'))        \n",
    "        elif _field in _fields:\n",
    "            continue\n",
    "        if not _file: \n",
    "            _file = _dimension_file\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_file, _line)\n",
    "    os.close(_fact_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['UniProtKB'], _fact_path, _dimension_path)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 33: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6cfbec2f33dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munitprot_parser\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mupp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mupp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessUnitProtData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UniProtKB'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UniProtKB'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Cours\\M2DK\\projects\\Bioinformatics\\code\\unitprot_parser.py\u001b[0m in \u001b[0;36mprocessUnitProtData\u001b[1;34m(filepath, path, uniqueFile)\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"creating path \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadFileData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m     \u001b[0mgenerateInsertQueries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniqueFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Cours\\M2DK\\projects\\Bioinformatics\\code\\unitprot_parser.py\u001b[0m in \u001b[0;36mloadFileData\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mcancer_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AC'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'KW'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bralt\\appdata\\local\\programs\\python\\python35-32\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 33: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import unitprot_parser as upp\n",
    "\n",
    "upp.processUnitProtData(_datasets['UniProtKB'], _datasets['UniProtKB'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to database\n",
    "def _connect(_configuration, _create = False):\n",
    "    _params = _configuration\n",
    "    print('Connecting to the database \\\"' + _configuration['database'] + '\\\" in PostgreSQL')\n",
    "    _connection = psycopg2.connect(**_params)\n",
    "    _connection.set_session(autocommit = True)\n",
    "    _cursor = _connection.cursor()\n",
    "    print('Version of PostgreSQL:')\n",
    "    _cursor.execute('SELECT version()')\n",
    "    _version = _cursor.fetchone()\n",
    "    print(_version)\n",
    "    _cursor.close()\n",
    "    # creating tables\n",
    "    if _create:\n",
    "        with _connection.cursor() as _cursor:\n",
    "            _cursor.execute(open('../database/database.sql', 'r').read())\n",
    "    return _connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating tables\n",
    "def _populate(_path, _connection):\n",
    "    with _connection.cursor() as _cursor:\n",
    "        _cursor.execute('SELECT current_database()')\n",
    "        _database = _cursor.fetchone()\n",
    "        print('Populating the database \\\"' + _database[0] + '\\\" using the file \\\"' + _path + '\\\"')\n",
    "        _cursor.execute(open(_path, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database \"bioinformatics\" in PostgreSQL\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (::1) and accepting\n\tTCP/IP connections on port 5432?\ncould not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ff5a1694d482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_create\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m _datasets = [\n\u001b[1;32m      4\u001b[0m     \u001b[0m_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Homo_sapiens.gene_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene2go'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9bdbdfa83399>\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(_configuration, _create)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Connecting to the database \\\"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_configuration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'database'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\\" in PostgreSQL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0m_cursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/psycopg2/__init__.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (::1) and accepting\n\tTCP/IP connections on port 5432?\ncould not connect to server: Connection refused\n\tIs the server running on host \"localhost\" (127.0.0.1) and accepting\n\tTCP/IP connections on port 5432?\n"
     ]
    }
   ],
   "source": [
    "_connection = _connect(_configuration, _create = True)\n",
    "\n",
    "_datasets = [\n",
    "    _datasets['Homo_sapiens.gene_info'],\n",
    "    _datasets['gene2go'],\n",
    "    _datasets['UniProtKB'],\n",
    "]\n",
    "\n",
    "# populating tables\n",
    "for _dataset in _datasets:\n",
    "    _populate(_dataset, _connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #\n",
    "    \n",
    "_configuration = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "_connection = _connect(_configuration, _create = False)\n",
    "\n",
    "_connection.cursor().execute('DROP DATABASE IF EXISTS \"bioinformatics\";')\n",
    "\n",
    "_connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
