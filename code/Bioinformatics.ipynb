{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to describe the extract, transform, load (ETL) process for gene-specific data from NCBI and UniProt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with Python 2.7.14. However, some external libraries are necessary to be include in the python enviroment if not yet done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# uncompressing files\n",
    "def _uncompress(_path):\n",
    "    with gzip.open(_path, 'rb') as _input, open('./' + os.path.splitext(os.path.basename(_path))[0], 'w') as _output:\n",
    "        # moving and renaming files\n",
    "        shutil.copyfileobj(_input, _output) \n",
    "        return os.path.basename(_output.name)\n",
    "    \n",
    "# reading files    \n",
    "def _read(_path):\n",
    "    return pandas.read_csv(_path, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describe the datasets used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datasets = '../datasets/'\n",
    "\n",
    "_datasets = {\n",
    "    'Homo_sapiens.gene_info': _datasets + 'NCBI/Homo_sapiens.gene_info.gz',\n",
    "    'gene2go': _datasets + 'NCBI/gene2go.gz',\n",
    "    'UniProtKB': _datasets + 'UniProt/uniprot-cancer+AND+reviewed%3Ayes+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B--.txt.gz'\n",
    "}\n",
    "\n",
    "# uncompressing files\n",
    "for _index in _datasets:\n",
    "    _datasets[_index] = _uncompress(_datasets[_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homo_sapiens.gene_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['Homo_sapiens.gene_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gene2go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['gene2go'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target taxonomy of this project is the Homo sapiens (Human) that holds the taxon indentifier 9606."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_taxon = 9606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniProt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UniProtKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = _read(_datasets['UniProtKB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows a sample of the contents of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below describe the schema of the target database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Database schema](../database/database.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration file\n",
    "_configuration = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'bioinformatics',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "with open('database.ini', 'w') as _output:\n",
    "    _output.write('[postgresql]' + '\\n')\n",
    "    # writing configuration file\n",
    "    for _parameter in _configuration:\n",
    "        _output.write(_parameter + '=' + _configuration[_parameter] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# converting any value to a string insertable into PostgreSQL\n",
    "def _convert(_data):\n",
    "    if isinstance(_data, int):\n",
    "        return str(_data)\n",
    "    if isinstance(_data, datetime):\n",
    "        # converting a date value to a string insertable into PostgreSQL\n",
    "        return '\\'' + datetime.strftime(_data, \"%Y-%m-%d\") + '\\''\n",
    "    return '\\'' + _data.strip().replace('\\'', '\\'\\'') + '\\''\n",
    "\n",
    "# skipping useless lines\n",
    "def _skip(_line, _fields):\n",
    "    _line = _line.strip()\n",
    "    # skipping comments\n",
    "    if _line.startswith('#'):\n",
    "        return None\n",
    "    # skipping comments  \n",
    "    _parts = _line.split('\\t')\n",
    "    # skipping lines with bad formatation\n",
    "    if len(_parts) != len(_fields):\n",
    "        return None\n",
    "    # skipping lines does not satisfy conditions\n",
    "    if not _parts[0].startswith(str(_taxon)): \n",
    "        return None\n",
    "    return _parts\n",
    "\n",
    "# replacing values under specific conditions\n",
    "def _replace(_text, _data = None):\n",
    "    _text.strip()\n",
    "    # applying user-provided function to values\n",
    "    if isinstance(_data, tuple):\n",
    "        _text = _data[0](_text, _data[1])\n",
    "    # expading value to a list of values\n",
    "    if isinstance(_data, list):\n",
    "        for _part in _text.split('|'):\n",
    "            _data.append(_replace(_part))\n",
    "        return _data\n",
    "    # providing meaning for null values\n",
    "    if _text == '-':\n",
    "        _text = 'NOT AVAILABLE'\n",
    "    # converting any value to a string insertable into PostgreSQL\n",
    "    return _convert(_text)\n",
    "\n",
    "# applying functions to lines\n",
    "def _process(_line, _data, _fields):\n",
    "    _parts = _skip(_line, _fields)\n",
    "    # skipping useless lines\n",
    "    if _parts is None:\n",
    "        return None\n",
    "    # replacing values under specific conditions\n",
    "    for _index, _part in enumerate(_parts):\n",
    "        _data[_index] = _replace(_part, _data[_index])\n",
    "    return _data\n",
    "\n",
    "# creating SQL INSERT INTO statements\n",
    "def _strings(_data, _fields, _relation, _prefix):\n",
    "    _lines = []\n",
    "    _line = ''\n",
    "    _arrays = []\n",
    "    for _index, _value in enumerate(_data):\n",
    "        # adding values to new SQL INSERT INTO statements\n",
    "        if isinstance(_value, list):\n",
    "            _arrays.append(_index)\n",
    "        # appending values to SQL INSERT INTO statement\n",
    "        elif isinstance(_value, str):\n",
    "            _line = _line + ', ' + _value\n",
    "        else :\n",
    "            # warning the user about errors in the SQL INSERT INTO statement\n",
    "            _line = '=>' + _fields[_index] + '\\t ERROR (' + type(_value)\n",
    "            _arrays = []\n",
    "            break\n",
    "    _lines.append('INSERT INTO ' + _relation + ' VALUES (' + _line[2:] + ');\\n') \n",
    "    # adding new SQL INSERT INTO statements\n",
    "    for _index in _arrays:\n",
    "        # adding new SQL INSERT INTO statement\n",
    "        for _value in _data[_index]:\n",
    "            _lines.append('INSERT INTO ' + _fields[_index] + ' VALUES (' + _prefix + ', ' + _value + ');\\n') \n",
    "    return _lines\n",
    "\n",
    "# providing consistency to SQL INSERT INTO statements\n",
    "def _consistency(_data, _fields, _relation):\n",
    "    _line = ''\n",
    "    # appending values to SQL INSERT INTO statement\n",
    "    for _value in _data:\n",
    "        _line = _line + ', ' + _value\n",
    "    _line = 'INSERT INTO ' + _relation + ' SELECT ' + _line[2:] + ' FROM ' + _relation + ' WHERE NOT EXISTS (SELECT TRUE FROM ' + _relation + ' WHERE '\n",
    "    # appending clausules to SQL INSERT INTO statement\n",
    "    for _field, _value in zip(_fields, _data):\n",
    "        _line = _line + _field + ' = ' + _value + ' AND '\n",
    "    return _line[:-5] + ') LIMIT 1;\\n'\n",
    "\n",
    "# removing duplicate lines\n",
    "def _deduplicate(_dataset, *_files):\n",
    "    _output, _path = tempfile.mkstemp()\n",
    "    for _file in _files:\n",
    "        with open(_file, 'r') as _input:\n",
    "            for _line, _group in itertools.groupby(sorted(_input)):\n",
    "                os.write(_output, _line)\n",
    "    os.close(_output)\n",
    "    shutil.move(_path, _dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_datasets['Homo_sapiens.gene_info'], 'r') as _input:\n",
    "    _fact_file, _fact_path = tempfile.mkstemp()\n",
    "    _dimension_file, _dimension_path = tempfile.mkstemp()\n",
    "    _fields = [\n",
    "        'taxonomy_id',\n",
    "        'gene_id',\n",
    "        'symbol',\n",
    "        'locus_tag',\n",
    "        'synonym',\n",
    "        'db_xref',\n",
    "        'chromosome',\n",
    "        'map_location',\n",
    "        'description',\n",
    "        'type_of_gene',\n",
    "        'symbol_from_nomenclature_authority',\n",
    "        'full_name_from_nomenclature_authority',\n",
    "        'nomenclature_status',\n",
    "        'other_designation',\n",
    "        'modification_date',\n",
    "        'feature_type'\n",
    "    ]\n",
    "    for _line in _input:\n",
    "        _data = [\n",
    "            (int, 10),\n",
    "            (int, 10),\n",
    "            None,\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [], \n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            [],\n",
    "            (datetime.strptime, '%Y%m%d'), \n",
    "            []\n",
    "        ]\n",
    "        # applying functions to lines\n",
    "        _data = _process(_line, _data, _fields)\n",
    "        if _data is None:\n",
    "            continue\n",
    "        _data.insert(0, _data.pop(1)) # gene_id\n",
    "        # creating SQL INSERT INTO statements\n",
    "        _lines = _strings(_data, _fields, 'ncbi', _data[0])\n",
    "        os.write(_fact_file, _lines.pop(0))\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_dimension_file, _line)\n",
    "    os.close(_fact_file)\n",
    "    os.close(_dimension_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['Homo_sapiens.gene_info'], _fact_path, _dimension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_datasets['gene2go'], 'r') as _input:\n",
    "    _fact_file, _fact_path = tempfile.mkstemp()\n",
    "    _dimension_file, _dimension_path = tempfile.mkstemp()\n",
    "    _fields = [\n",
    "        'taxonomy_id',\n",
    "        'gene_id',\n",
    "        'go_id',\n",
    "        'evidence',\n",
    "        'qualifier',\n",
    "        'go_term',\n",
    "        'pubmed',\n",
    "        'category'\n",
    "    ]\n",
    "    _labels = [\n",
    "        'go_id',\n",
    "        'evidence',\n",
    "        'go_term',\n",
    "        'category'\n",
    "    ]\n",
    "    for _line in _input:\n",
    "        _data = [\n",
    "            None,\n",
    "            (int, 10),\n",
    "            None,\n",
    "            [],\n",
    "            None,\n",
    "            [],\n",
    "            [],\n",
    "            None\n",
    "        ]\n",
    "        # applying functions to lines\n",
    "        _data = _process(_line, _data, _fields)\n",
    "        if _data is None:\n",
    "            continue\n",
    "        _data.pop(0) # 'taxonomy_id'\n",
    "        for _index, _value in enumerate(_data[2]): # 'evidence'\n",
    "            _data[2][_index] = _value + ', ' + _data[3] + ', ' + _replace('-') \n",
    "        _data.pop(3) # 'qualifier'\n",
    "        _data.pop(4) # 'pubmed'\n",
    "        gene_id = _data.pop(0) # 'gene_id'\n",
    "        # creating SQL INSERT INTO statements\n",
    "        _lines = _strings(_data, _labels, 'go', _data[0])\n",
    "        os.write(_fact_file, _lines.pop(0))\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_dimension_file, _line)\n",
    "        # writing SQL statements\n",
    "        for _line in _strings([gene_id, _data[0]], [_fields[1], _labels[0]], 'ncbi_go', None):\n",
    "            os.write(_dimension_file, _line)\n",
    "    os.close(_fact_file)\n",
    "    os.close(_dimension_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['gene2go'], _fact_path, _dimension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_datasets['UniProtKB'], 'r') as _input:\n",
    "    _uniprot_file, _uniprot_path = tempfile.mkstemp()\n",
    "    _go_file, _go_path = tempfile.mkstemp()    \n",
    "    _uniprot_dimension_file, _uniprot_dimension_path = tempfile.mkstemp()\n",
    "    _go_dimension_file, _go_dimension_path = tempfile.mkstemp()    \n",
    "    _fields = [\n",
    "        'ID', \n",
    "        'AC', \n",
    "        'DE', \n",
    "        'GN', \n",
    "        'KW', \n",
    "        'DR',\n",
    "    ]\n",
    "    _labels = [\n",
    "        'protein_id',\n",
    "        'accession_number',\n",
    "        None,\n",
    "        None,\n",
    "        'keyword'\n",
    "    ]\n",
    "    _id = None\n",
    "    _flag = '-'\n",
    "    for _line in _input:        \n",
    "        _line = _line.strip()\n",
    "        if _line.startswith('#'):\n",
    "            continue\n",
    "        _parts = _line.split()\n",
    "        if len(_parts) < 2:\n",
    "            continue\n",
    "        _field = _parts.pop(0)\n",
    "        if _field not in _fields :\n",
    "            continue\n",
    "        _data = [_id]\n",
    "        _lines = []\n",
    "        _file = None\n",
    "        if _line[-1] == '.':\n",
    "            _line = _line[:-1]\n",
    "        if _id == None or _field == _fields[0] and _convert(_parts[0]) != _id:\n",
    "            if _id:\n",
    "                # creating SQL INSERT INTO statements\n",
    "                for _line in _strings([_id, _flag], [_labels[0], '_flag'], 'uniprot', None):\n",
    "                    os.write(_uniprot_file, _line)\n",
    "            _id = _convert(_parts[0]) \n",
    "            _flag = _convert('-') \n",
    "        elif _field == _fields[1] or _field == _fields[4]:\n",
    "            _parts = _line.split(';')\n",
    "            _parts[0] = _parts[0][2:]\n",
    "            _parts[0] = _parts[0].strip()\n",
    "            _index = _fields.index(_field)\n",
    "            _label = _labels[_index]\n",
    "            for _part in _parts:\n",
    "                if _part != '':\n",
    "                    _part = _convert(_part)\n",
    "                    # creating SQL INSERT INTO statements\n",
    "                    _lines = _strings([_id, _part], [_labels[0], _label], _label, None)\n",
    "        elif _field == _fields[5] and _parts.pop(0) == 'GO;':\n",
    "            _parts = _line.split(';')\n",
    "            _parts.pop(0)\n",
    "            _data = [_convert(_parts.pop(0)), _id]\n",
    "            # \n",
    "            for _line in _strings(_data, ['go_id', _labels[0]], 'go_uniprot', None):\n",
    "                os.write(_uniprot_dimension_file, _line)\n",
    "            _data.pop()\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split(':')\n",
    "            _category = _values[0].strip()\n",
    "            if _category == 'P':\n",
    "                _data.append(_convert('Process'))\n",
    "            elif _category == 'F':\n",
    "                _data.append(_convert('Function'))\n",
    "            elif _category == 'C':\n",
    "                _data.append(_convert('Component'))\n",
    "            else:\n",
    "                assert False\n",
    "            # \n",
    "            for _line in _consistency(_data, ['go_id', 'category'], 'go'):\n",
    "                os.write(_go_file, _line)\n",
    "            _data.pop()\n",
    "            _data.append(_convert(_values[1]))\n",
    "            # providing consistency to SQL INSERT INTO statements\n",
    "            _lines.append(_consistency(_data, ['go_id', 'go_term'], 'go_term'))        \n",
    "            _data.pop()\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split(':')\n",
    "            _data.append(_convert(_values[0]))\n",
    "            _data.append(_replace('-'))\n",
    "            _data.append(_convert(_values[1]))\n",
    "            # providing consistency to SQL INSERT INTO statements\n",
    "            _lines.append(_consistency(_data, ['go_id', 'evidence', 'qualifier', 'source'], 'evidence'))        \n",
    "            _file = _go_dimension_file\n",
    "        elif _field == _fields[5] and _parts[0] in ['RecName:', 'AltName:']:\n",
    "            _parts = _line.split(':')\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split()\n",
    "            _data.append(_convert(_values[1]))\n",
    "            _values = _parts.pop(0)\n",
    "            _values = _values.split('=')\n",
    "            _data.append(_convert(_values[0]))\n",
    "            _data.append(_convert(_values[1][:-1]))\n",
    "        elif _field == _fields[5] and _parts.pop(0) == 'Flags:':\n",
    "            _flag = _parts[0];\n",
    "            _flag = _convert(_flag[:-1])\n",
    "        elif _field in _fields:\n",
    "            continue\n",
    "        if not _file: \n",
    "            _file = _uniprot_dimension_file\n",
    "        # writing SQL statements\n",
    "        for _line in _lines:\n",
    "            os.write(_file, _line)\n",
    "    for _line in _strings([_id, _flag], [_labels[0], '_flag'], 'uniprot', None):\n",
    "        os.write(_uniprot_file, _line)\n",
    "    os.close(_uniprot_file)\n",
    "    # removing duplicate lines\n",
    "    _deduplicate(_datasets['UniProtKB'], _uniprot_path, _go_path, _go_dimension_path, _uniprot_dimension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import unitprot_parser as upp\n",
    "\n",
    "upp.processUnitProtData(_datasets['UniProtKB'], _datasets['UniProtKB'], True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to database\n",
    "def _connect(_configuration, _create = False):\n",
    "    _params = _configuration\n",
    "    print('Connecting to the database \\\"' + _configuration['database'] + '\\\" in PostgreSQL')\n",
    "    _connection = psycopg2.connect(**_params)\n",
    "    _connection.set_session(autocommit = True)\n",
    "    _cursor = _connection.cursor()\n",
    "    print('Version of PostgreSQL:')\n",
    "    _cursor.execute('SELECT version()')\n",
    "    _version = _cursor.fetchone()\n",
    "    print(_version)\n",
    "    _cursor.close()\n",
    "    # creating tables\n",
    "    if _create:\n",
    "        with _connection.cursor() as _cursor:\n",
    "            _cursor.execute(open('../database/database.sql', 'r').read())\n",
    "    return _connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating tables\n",
    "def _populate(_path, _connection):\n",
    "    with _connection.cursor() as _cursor:\n",
    "        _cursor.execute('SELECT current_database()')\n",
    "        _database = _cursor.fetchone()\n",
    "        print('Populating the database \\\"' + _database[0] + '\\\" using the file \\\"' + _path + '\\\"')\n",
    "        _cursor.execute(open(_path, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_connection = _connect(_configuration, _create = True)\n",
    "\n",
    "_datasets = [\n",
    "    _datasets['Homo_sapiens.gene_info'],\n",
    "    _datasets['gene2go'],\n",
    "    _datasets['UniProtKB'],\n",
    "]\n",
    "\n",
    "# populating tables\n",
    "for _dataset in _datasets:\n",
    "    _populate(_dataset, _connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #\n",
    "'''    \n",
    "_configuration = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "_connection = _connect(_configuration, _create = False)\n",
    "\n",
    "_connection.cursor().execute('DROP DATABASE IF EXISTS \"bioinformatics\";')\n",
    "\n",
    "_connection.close()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
